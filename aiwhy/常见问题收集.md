# interestingresources
collect something interesting

you can go to github trends to explore new interesting things on github

new pull request is welcome

介绍一下逻辑回归，这里问得非常细致。包括他的训练过程，他是怎么得到权重，怎么更新权重，为什么要选择梯度方向进行优化，从数学角度来说一下（有必要用自己的语言去整理和写一下代码）

l1和l2正则化，区别与应用场景

用数学语言解释梯度下降

knn与k-means的区别

分类指标

滤波知识：均值滤波、中值滤波、维纳滤波

问题：python数据类型中，不可变的有哪些，可变的有哪些
答：不可变的是元组，可变的是字典、列表

偏差、方差区别

模型比较复杂的时候，偏差和方差的变化：偏差变小，方差变大

1*1 卷积的作用：（我没有答出来） 面试官 补充到：主要是起到降维或者是获取到更多特征的作用

数据不平衡的时候的处理方法，比如说正样本多 https://blog.csdn.net/shenxiaoming77/article/details/72616333
1采样 上采样和下采样 上采样是把小众类复制多份，下采样是从大众类中剔除一些样本，或者说只从大众类中选取部分样本
2数据合成 用已有样本生成更多样本，这类方法在小数据场景下有很多成功案例
3加权 boosting

数据增强的方法

模型过拟合了怎么办：数据增强、修改模型、L1和L2正则化

KNN和K-means

L1和L2正则化的区别 L1趋向于产生少量的特征，其他特征为0，因为最优参数很大概率出现在坐标轴上，导致某一维的权重为0，产生稀疏矩阵。 L2趋向于选择更多的特征，这些特征会接近于0，最优的参数值很小概率出现在坐标轴上，每一维的参数不会是0.当最小化||w||时，就会使每一项趋向于0

1、ROC,AUC（没答出来）

2、gdbt、xgboost、随机森林有什么区别（没答出来）

3、激活函数

4、过拟合、欠拟合

笔试结束了之后，突如其来的面试 就赶着问，最后面试官给我的建议是让我找一个具体的点，去kaggle上动手做实验。 然后自己要有一个非常熟悉的点，比如就算是简单的LR，优缺点、应用场景也要弄得很清楚。

虚函数的作用：函数的作用是允许在派生类中重新定义与基类同名的函数，并且可以通过基类指针或引用来访问基类和派生类中的同名函数

栈和函数调用的关系

栈和队列的区别

介绍熟悉的机器学习算法，并推导svm

介绍gan

介绍堆排序

聊项目以及拓展

了解过deapfake吗

梯度消失的原因以及解决方法

为什么使用sigmoid激活函数会导致梯度消失

面试套路大概归结为下面流程：
1、先做一个简短的自我介绍
答：根据你项目的时间点结合你的学习历程简单介绍。比如研一上学的什么然后做了什么，研二学的什么然后做了什么。一定要简介别说细节，可以简单说效果提升。还有你获得过的国奖啊、校奖、acm啊都要说一下。
2、大体按照相关实习经历==发表过的论文（水准高的）>较水论文>top比赛=项目的顺序进行发问。这个东西就看你对自己项目的理解了，没人能帮得了。
3、手写代码，写不上来大概率gg，不过ai公司问的算法都挺easy。
4、一些你平时用的语法的知识 && 深度学习一些基础知识。（这些可以看牛客的面经，很清楚）

问了nms、极大值抑制、ssd、faster rcnn、实习项目

一面主要问你研究方向->实习->逮住一个点比如目标检测直接问到底（手写nms）。面试官很nice，文艺范。
二面直接刚了几个算法。。。回溯之类的（比网上见到头条面试要良心不少）。后面问了几个操作系统相关的知识。

商汤
面了很多c++和c（非科班怕是直接凉，也可能因为本人没论文。）。

1、微积分
1、SGD,Momentum,Adagard,Adam原理

2、L1不可导的时候该怎么办

3、sigmoid函数特性

作者：牛妹
链接：https://www.nowcoder.com/discuss/165930
来源：牛客网

2、统计学，概率论
a,b~U[0,1]，互相独立,求Max(a,b)期望
2、一个活动,n个女生手里拿着长短不一的玫瑰花,无序的排成一排,一个男生从头走到尾,试图拿更长的玫瑰花,一旦拿了一朵就不能再拿其他的,错过了就不能回头,问最好的策略?

3、问题：某大公司有这么一个规定：只要有一个员工过生日，当天所有员工全部放假一天。但在其余时候，所有员工都没有假期，必须正常上班。这个公司需要雇用多少员工，才能让公司一年内所有员工的总工作时间期望值最大？

4、切比雪夫不等式

5、一根绳子,随机截成3段,可以组成一个三角形的概率有多大

6、最大似然估计和最大后验概率的区别?

7、什么是共轭先验分布

8、概率和似然的区别

9.频率学派和贝叶斯学派的区别

10、0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器

11、Lasso的损失函数

12、Sfit特征提取和匹配的具体步骤

作者：牛妹
链接：https://www.nowcoder.com/discuss/165930
来源：牛客网

3、线性代数
1、求mk矩阵A和nk矩阵的欧几里得距离?

2、PCA中第一主成分是第一的原因?

3、欧拉公式

4、矩阵正定性的判断,Hessian矩阵正定性在梯度下降中的应用

5、概率题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数

6、讲一下PCA

7、拟牛顿法的原理

8、编辑距离

作者：牛妹
链接：https://www.nowcoder.com/discuss/165930
来源：牛客网

1、处理分类问题常用算法
1、交叉熵公式

2、LR公式

3 LR的推导，损失函数

4、逻辑回归怎么实现多分类

5 、SVM中什么时候用线性核什么时候用高斯核?

6、什么是支持向量机,SVM与LR的区别?

7.监督学习和无监督学习的区别

8.机器学习中的距离计算方法?

9、问题：朴素贝叶斯（naive Bayes）法的要求是？

10、问题：训练集中类别不均衡，哪个参数最不准确？

11、问题：你用的模型，最有挑战性的项目

12、问题：SVM的作用，基本实现原理；

13、问题：SVM的硬间隔，软间隔表达式；

14、问题：SVM使用对偶计算的目的是什么，如何推出来的，手写推导；

15、问题：SVM的物理意义是什么；

16、问题：如果给你一些数据集，你会如何分类（我是分情况答的，从数据的大小，特征，是否有缺失，分情况分别答的）；

17、问题： 如果数据有问题，怎么处理；

18、分层抽样的适用范围

19、LR的损失函数

20、LR和线性回归的区别

21 生成模型和判别模型基本形式，有哪些？

22 核函数的种类和应用场景。

23 分类算法列一下有多少种？应用场景。

24、给你一个检测的项目，检测罐装的可口可乐，瓶装的可口可乐作为负样本，怎么弄？

25.SVM核函数的选择

26.SVM的损失函数

27.核函数的作用

28.SVM为什么使用对偶函数求解

29.ID3,C4.5和CART三种决策树的区别

30.SVM和全部数据有关还是和局部数据有关?

31为什么高斯核能够拟合无穷维度

32、第二面完整推导了svm一遍，还有强化学习问的很多，dqn的各种trick了解多少，怎么实现知不知道。

33、SVM所有核函数的了解应用，SVM的损失函数

34、LR和SVM 区别

35、朴素贝叶斯基本原理和预测过程

36、LR推导

37、交叉熵，还有个什么熵不记得了。。。

38、LR公式

39、交叉熵公式

作者：牛妹
链接：https://www.nowcoder.com/discuss/165930
来源：牛客网

2、处理回归问题常用算法
1.L1和L2正则化的区别

2、问题：Loss Function有哪些，怎么用？

3、问题：线性回归的表达式，损失函数；

4、线性回归的损失函数

5、机器学习：知道哪些传统机器学习模型

作者：牛妹
链接：https://www.nowcoder.com/discuss/165930
来源：牛客网

3、处理聚类问题常用算法
1、什么是DBSCAN

2.k-means算法流程

3、LDA的原理

4、介绍几种机器学习的算法，我就结合我的项目经理介绍了些RF, Kmeans等算法。
5、KMeans讲讲，KMeans有什么缺点，K怎么确定
6、Kmeans
7、DBSCAN原理和算法伪代码，与kmeans，OPTICS区别

作者：牛妹
链接：https://www.nowcoder.com/discuss/165930
来源：牛客网

5、模型融合和提升的算法
1 bagging和boosting的区别

2、boosting和 bagging区别

3 XGBOOST和GDBT的区别

4.GDBT的原理,以及常用的调参参数

6、AdaBoost和GBDT的区别,AdaBoost和GBDT的区别

7 gbdt推导
8、boosting和bagging在不同情况下的选用
9、gbdt推导和适用场景
10、说一下gbdt的全部算法过程
11、rf和gbdt基分类器区别，里面的决策树分别长啥样，怎么剪枝
12、随机森林和 GBDT 的区别

13、xgboost的特征重要性计算

14 xgboost的正则项表达式

15 xgboost原理，怎么防过拟合
16、xgboost，rf，lr优缺点场景。。。
17、xgboost特征并行化怎么做的

18、 xgboost和lightgbm的区别和适用场景

6、其他重要算法
1、问题：HMM隐马尔可夫模型的参数估计方法是？

2、问题：Bootstrap方法是什么？

3、问题：如何防止过拟合？

4、 EM算法推导，jensen不等式确定的下界

作者：牛妹
链接：https://www.nowcoder.com/discuss/165930
来源：牛客网

1、Scikit-learn
1、Focal Loss 介绍一下

2 过拟合的解决方法

3 方差偏差的分解公式

4、问题：对应时间序列的数据集如何进行交叉验证？

5、问题：正负样本不平衡的解决办法？评价指标的参考价值？

6、迁移学习

7、数据不平衡怎么办?

8、AUC的理解

9、AUC的计算公式

10、生成模型和判别模型的区别

11、过拟合的解决方法

12、特征选择怎么做

13、怎么防止过拟合

14、L1和L2正则

15、ID3树用什么指标选择特征

16、特征工程的问题

17、给了个链接线上写代码，要求写读文本、文本预处理、特征提取和建模的基本过程，不过写到特征就没写了

18、softmax公式

19、softmax公式

2、Libsvm
1、 检测20类物体，多少张训练集，怎么训练

2、 lightgbm优势

作者：牛妹
链接：https://www.nowcoder.com/discuss/165930
来源：牛客网

四、深度学习
1 BatchNormalization的作用

2、梯度消失

3、循环神经网络，为什么好?

4 什么是GroupConvolution

5.什么是RNN

6.训练过程中,若一个模型不收敛,那么是否说明这个模型无效?导致模型不收敛的原因有哪些?

7.图像处理中锐化和平滑的操作

8.VGG使用3*3卷积核的优势是什么?

9.Relu比Sigmoid的效果好在哪里?

10、问题：神经网络中权重共享的是？

11、问题：神经网络激活函数？

12、问题：在深度学习中，通常会finetuning已有的成熟模型，再基于新数据，修改最后几层神经网络权值，为什么？

13、问题：画GRU结构图

14、Attention机制的作用

15、Lstm和Gru的原理

16、什么是dropout

17、LSTM每个门的计算公式

18、HOG算法原理

19、DropConnect的原理

20 深度学习了解多少，有看过底层代码吗？caffe,tf?

21、除了GMM-HMM，你了解深度学习在语音识别中的应用吗？

22、用过哪些移动端深度学习框架？

作者：牛妹
链接：https://www.nowcoder.com/discuss/165930
来源：牛客网

18、HOG算子是怎么求梯度的

1、BN层的作用，为什么要在后面加伽马和贝塔，不加可以吗

2、梯度消失，梯度爆炸的问题，

3、Adam

4、attention机制

5、RNN梯度消失问题,为什么LSTM和GRU可以解决此问题

6、GAN网络的思想

7、1*1的卷积作用

8、怎么提升网络的泛化能力

9、什么是seq2seq model

10、激活函数的作用

11、为什么用relu就不用sigmoid了

12、讲一下基于WFST的静态解码网络的语音识别流程？

13、目标检测了解吗，Faster RCNN跟RCNN有什么区别

14、SPP，YOLO了解吗？

15、梯度消失梯度爆炸怎么解决

16、RNN容易梯度消失，怎么解决？

17、LSTM跟RNN有啥区别

18、卷积层和池化层有什么区别

19、 防止过拟合有哪些方法

20、dropout咋回事讲讲

21、relu

22、神经网络为啥用交叉熵。

23、注意力公式

24、论文flow情况

25、Flappy.Bird开发者,怎么利用DNQ方法强化学习你的游戏AI

26、LeNet-5结构

27、推导LSTM正向传播和单向传播过程

28、LSTM原理，与GRU区别

29、DNN的梯度更新方式

30、 CNN为什么比DNN在图像识别上更好

31、现场用collabedit写代码，一个怪异的归并算法。。。之前没遇到过，直接把归并写出来，但是说复杂度太高，优化了三遍还不行，最后说出用小顶堆解决了。。。

32、LSTM和Naive RNN的区别

33、神经网络为啥用交叉熵。

34、注意力公式

35、Inception Score 评价指标介绍

36、使用的 CNN 模型权重之间有关联吗？

37、CycleGAN 原理介绍一下

38、训练 GAN 的时候有没有遇到什么问题

39、百度实习：CPM 模型压缩怎么做的？有压过 OpenPose 吗？

40、用过哪些 Optimizer，效果如何

41、图像基础：传统图像处理方法知道哪些，图像对比度增强说一下

42、介绍一下图像的高频、低频部分，知道哪些图像补全的方法

43、百度实习：模型压缩的大方向。CPM 模型怎么压缩的，做了哪些工作？

44、Depthwise 卷积实际速度与理论速度差距较大，解释原因。

45、RetinaNet 的大致结构画一下

46、RetinaNet为什么比SSD效果好

bootstrap数据是什么意思？
有放回地从总共N个样本中抽样n个样本
题目解析：
概念题，bootstrap又称自展法，是用小样本估计总体值的一种非参数方法，基本思想是生成一系列bootstrap伪样本，每个样本是初始数据有放回抽样

下面有关序列模式挖掘算法的描述，错误的是？
https://www.nowcoder.com/questionTerminal/2aefc7fe5f88414997997196089c65c6?orderByHotValue=2&pos=13&mutiTagIds=631

作者：小哲AI
链接：https://www.nowcoder.com/discuss/765855?source_id=discuss_experience_nctrack&channel=-1
来源：牛客网
1. 机器学习
梯度下降法与牛顿法

感知机的计算公式

K近邻中kd树的思路

朴素贝叶斯的推导以及假设条件

ID3，C4.5，CART决策树的计算流程

Logistic回归的极大似然推导

SVM的推导，KKT条件，原问题与对偶问题的联系

Adaboost，梯度提升决策树，GBDT，XGboost

L1正则化为什么能够获得稀疏解

作者：小哲AI
链接：https://www.nowcoder.com/discuss/765855?source_id=discuss_experience_nctrack&channel=-1
来源：牛客网

2. 深度学习
RCNN系列算法的演进过程，重点详细介绍Faster RCNN算法

YOLO系列算法（v1-v4）的演进以及每个算法的特点

YOLOv3的主要改进

YOLOv4的主要改进

SSD系列算法（SSD，DSSD等算法）

two-stage算法主要慢在哪儿

ROIpooling，RoiAlign的计算

RetinaNet解决什么问题

Focal Loss的计算

Batch Normalization的作用

BN的计算公式

BN中偏移因子与缩放因子的作用

训练和测试时BN的不同点

多卡的BN如何实现同步（一次同步与两次同步）

Normalization的几种形式的计算（BN，Layer Normalization，Instance Normalization，Group Normalization）

DNN的反向传播推导

CNN的反向传播

池化层的反向传播

img2col的计算

ResNet到底解决了什么问题

感受野计算

普通卷积，Group卷积，深度可分离卷积的计算量与参数量

IOU计算（代码）

NMS（代码）

soft-NMS（代码）

MobileNet与shuffleNet特点

MobileNetv1与v2的改进

K-means聚类anchor的思路以及代码

anchor-free的算法都知道哪些

DETR算法介绍一下

FPN为何能够提升小目标的精度

Softmax与sigmoid计算公式

softmax交叉熵损失的梯度

分类为何采用交叉熵损失而不是均方差损失

TP、TF、FP、FN的计算

precision、recall、F1score、accuracy的计算

ROC、AUC、MAP的计算

各种回归损失的计算以及优缺点（L1loss，L2loss，smooth L1loss，IOU Loss， GIOULoss，DIOUloss，CIOULoss）

带有warmup的cosine学习率计算

说说了解到的几种分割算法

Transformer的原理以及VIT的了解

各种优化器的计算公式（GD，SGD，batch GD，SGD+momentum，NAG，AdaGrad，RMSProp，Adam）

CNN的平移不变性的理解

Dropout具体是怎么做的，训练和测试时有什么不一样

神经网络的深度和宽度理解

SIFT与HOG

Transformer相比较CNN的优缺点

雅可比矩阵与海塞阵（一阶导与二阶导）

深度可分离卷积的优缺点，Mobilenet是否一定快

Faster RCNN回归的公式。

权重初始化的方案（Xavier，kaiming_normal）

数据增广的常用方案

数据类别不均衡的解决方案

半监督算法

自监督算法

自监督中的崩溃解问题的理解

Triplet Loss

3. 图像处理
各种图像边缘检测算子（sober算子，Robert算子，Prewitt算子，Laplace算子，canny算子）

低通高通滤波器

降噪

形态学处理（腐蚀以及膨胀）

图像量化

4. linux常用命令
查找文件命令（whereis，which，find，grep，locate）

查看进程

杀死进程

ssh，scp

nohup &

文件权限

5. c++基础
c++编译过程

静态链接库与动态链接区别

cmakelist文件

6. python基础
python多线程，多进程

python的GIL

生成器与迭代器

装饰器

深浅拷贝

作者：吃肉的星星
链接：https://www.nowcoder.com/discuss/765025?source_id=discuss_experience_nctrack&channel=-1
来源：牛客网
(2) 机器学习题：模型的方差和偏差是什么，怎么减少bias和var:(蛮经典的一题，不过深度的可能没怎么接触)
要点参考：
Error=bias+var+noise
方差：不同训练集训练的模型对相同测试集上输出的差异。
偏差：模型输出和真实结果的差异
Baging减少方差，boosting减少偏差
二者权衡：高偏差，低方差的模型

作者：吃肉的星星
链接：https://www.nowcoder.com/discuss/765025?source_id=discuss_experience_nctrack&channel=-1
来源：牛客网

（3）BN：太经典了。。。。考了好多次了。。。。
GN,IN,LN的关系：
https://uploadfiles.nowcoder.com/images/20211003/798824856_1633246967764/E483215212151A5CF629E0627D3A25CC
GN介于LN,IN，GN是基于BN在小batch上效果不好应运而生。LN相当于GN中分组为G=1   ，IN相当于GN中分组G=C.可以具体展开介绍包括沿着哪个方向归一化，归一化后的维度（N,C,H,W）各是多少
敲黑板：伪代码要会写！

追问：Transforemr中用LN，为什么？改成IN会变成什么？

作者：吃肉的星星
链接：https://www.nowcoder.com/discuss/765025?source_id=discuss_experience_nctrack&channel=-1
来源：牛客网
（4）解决梯度消失：、
要点参考：
1.grad_clip
2.Relu→Sigmoid
3.resnet
4.BN
追问： Relu→Sigmoid，为什么可以缓解梯度消失呢？Relu在0点可导性，
以及对于激活函数的要求是什么？
要点参考：
主要是因为它们gradient特性不同。sigmoid和的gradient在饱和区域非常平缓，接近于0，很容易造成梯度消失的问题，减缓学习的收敛速度。这个现象在网络层数多的时候尤其明显，是加深网络结构的主要障碍之一（ 加分点：介绍Resnet的改进）。相反，Relu的gradient大多数情况下 是常数，有助于解决深层网络的收敛问题。
在0点不可导，（回去复习高数Hhh）但是会 人为地设置此点的导数为0，顺便解释了一下可导性和可微的区别
要求：非线性，处处可微，非饱和，单调， zero-centered，最好还 计算简单hhh（ 加分点：介绍Mobilenet relu &relu 6的改进）。
同时举了个反例sin x，面试官点了点头，又说：不过这几年还真有人用了？？
我：？？？？？
然后结束了hhh，比较基础

作者：吃肉的星星
链接：https://www.nowcoder.com/discuss/765025?source_id=discuss_experience_nctrack&channel=-1
来源：牛客网
基础题：
1.常用的回归损失函数Timeline：
要点参考：
A:L1系列：L1,L2 Loss，SmoothL1 loss（重点介绍：为什么小的梯度用L2,大梯度用L1）
B:IOU系列 IOU ;Generalized IOU; Distance IOU;Compete IOU ~重点介绍各自的缺点和后续采用什么思路去改进的

2.Yolo系列时间线（问的很深，问到了yoloX和对于当前Yolo系列的评价）
要点参考：
（1）Yolov1:FC预测坐标绝对值，7x7 grid预测（相当于最初的anchor free）
（2）Yolov2：用Conv预测坐标的偏移量，BN，先验框的设置
（3）Yolov3：Darknet-53(参考了resnet的残差结构)，参考了FPN的多尺度预测，K-means
(4) Yolov4: 太多了。。。。CSPDarknet ,Mish激活函数，Mosaic增强等
**（5）Yolox ：AB→AF，sim OTA，Decoupled-head 这个是今年（2021的旷视的）的一个热点，建议好好阅读这篇paper，很佩服这篇文章的作者，respect!

然后分享一下我的view:Yolo从一开始到现在，由v1的AF到AB再到AF，其中不论的anchor-match,data aug,backbone等都有了很大的变化 ，包括v3后的作者推出了cv界，更是没有了统一的界定了。严格说yolo。我个人认为：（1）yolo目前始终注重speed和accrucy的trade-off!这个应该可以作为它的一个中心思想（2）始终One-stage(废话)

3.量化做过吗？TensorRT的推理加速的原理是什么？

4.Focal loss
要点参考：
（1）前置：CE Loss介绍
one-stage vs two-stage：没有rpn直接做回归，太多noise
Loss收敛:大部分是简单负样本，困难负样本（是背景预测为前景，而且分数还挺高的anchor）的学习收到抑制。
（2）α控制正负样本 β控制难易样本，控制的过程，公式分析和介绍
（3）相似应用：OHEM  IoU-balanced sampling等

5.解释一下归并排序吧~
要点：分治思想，递归拆分，栈实现两个有序数组的排序（双指针问题），复杂度分析（排序n递归拆分logn）时间复杂度nlogn，空间复杂度1
写了个伪代码~

反问：顺丰的CV业务~

Bagging：独立的集成多个模型，每个模型有一定的差异，最终综合有差异的模型的结果，获得学习的最终的结果；
Boosting（增强集成学习）：集成多个模型，每个模型都在尝试增强（Boosting）整体的效果；
Stacking（堆叠）：集成 k 个模型，得到 k 个预测结果，将 k 个预测结果再传给一个新的算法，得到的结果为集成系统最终的预测结果；
Bagging和Boosting的区别：

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

5）bagging是减少variance，而boosting是减少bias

典型的神经网络其训练流程是将输入通过网络进行正向传导，然后将误差进行反向传播，Dropout就是针对这一过程之中，随机地删除隐藏层的部分单元，进行上述过程。
步骤为：
1）随机删除网络中的一些隐藏神经元，保持输入输出神经元不变；
2）将输入通过修改后的网络进行前向传播，然后将误差通过修改后的网络进行反向传播；
3）对于另外一批的训练样本，重复上述操作。
他的作为从Hinton的原文以及后续的大量实验论证发现，dropout可以比较有效地减轻过拟合的发生，一定程度上达到了正则化的效果。

A:Boosting方法是一种用来提高弱分类算法准确度的方法,这种方法通过构造一个预测函数系列,然后以一定的方式将他们组合成一个预测函数。B：Bagging同样是弱分类器组合的思路，它首先随机地抽取训练集（training set），以之为基础训练多个弱分类器。然后通过取平均，或者投票(voting)的方式决定最终的分类结果。因为它随机选取训练集的特点，Bagging可以一定程度上避免过渡拟合(overfit)。
C:stacking：它所做的是在多个分类器的结果上，再套一个新的分类器。这个新的分类器就基于弱分类器的分析结果，加上训练标签(training label)进行训练。一般这最后一层用的是LR。
D:Sammon Mapping降维算法。

XGBOOST是如何进行优化的？ XGBOOST和GBDT的区别与联系？
讲一下SVR的大概思路？ 能手一下SVR吗？
梯度反向传播讲一下？ 能不能现场推个公式？
Attention和Transformer是怎么实现的？
介绍几个Loss函数 Hinge loss的优缺点是什么？
支持向量是什么意思？